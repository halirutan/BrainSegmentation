{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Brain Segmentation \u00b6 Welcome to the Brain Segmentation project documentation. Overview \u00b6 This project provides tools and algorithms for brain segmentation tasks. Brain segmentation is the process of identifying and isolating different structures within brain images, which is crucial for various medical and research applications. Here, we aim to improve segmentation results for resolutions higher than 1 mm and to better handle typical bias artifacts we see in 7T scans. To achieve that, we will create synthetic training datasets tailored to the resolution and contrasts we have. We plan to train different neural network model architectures and verify their performance against the UltraCortex dataset's manual segmentations. We hope that by training specific contrasts instead of being contrast-agnostic, the models can learn unique features of our images and segmentation on high-contrast images can be improved significantly. The approach consists of the following steps: Downloading large datasets and create (good-enough) full-head segmentations that serve as label training maps . Training label maps are used to create synthetic training pairs that contain various forms of augmentation and use a sampling model that creates \"fake\" MRI images that fit the segmentation labels perfectly. Creating the actual training data includes upsampling the training label maps to the target resolution. Additionally, a statistical contrast analysis of the MRI images that we want to segment is performed, which results in detailed information about how to sample different regions of the synthetic MRI images. With this, it becomes possible to generate arbitrary many randomly sampled and randomly augmented training pairs. Selecting and training a neural network model for high-resolution data brings its own set of challenges, such as dealing with memory constraints on today's GPUs. We will consider different strategies from different angles like patchwise or sliding window training, getting hold of NVidia GPUs larger than the 48GB available, or training on AMD APUs which have shared CPU/GPU memory. Once the training is done, we will verify the quality of the segmentation with different measures against the manual segmentations provided in the UltraCortex dataset. Getting Started \u00b6 Installation Contributing Features \u00b6 Scale label images for brain segmentation More features coming soon...","title":"Home"},{"location":"#brain-segmentation","text":"Welcome to the Brain Segmentation project documentation.","title":"Brain Segmentation"},{"location":"#overview","text":"This project provides tools and algorithms for brain segmentation tasks. Brain segmentation is the process of identifying and isolating different structures within brain images, which is crucial for various medical and research applications. Here, we aim to improve segmentation results for resolutions higher than 1 mm and to better handle typical bias artifacts we see in 7T scans. To achieve that, we will create synthetic training datasets tailored to the resolution and contrasts we have. We plan to train different neural network model architectures and verify their performance against the UltraCortex dataset's manual segmentations. We hope that by training specific contrasts instead of being contrast-agnostic, the models can learn unique features of our images and segmentation on high-contrast images can be improved significantly. The approach consists of the following steps: Downloading large datasets and create (good-enough) full-head segmentations that serve as label training maps . Training label maps are used to create synthetic training pairs that contain various forms of augmentation and use a sampling model that creates \"fake\" MRI images that fit the segmentation labels perfectly. Creating the actual training data includes upsampling the training label maps to the target resolution. Additionally, a statistical contrast analysis of the MRI images that we want to segment is performed, which results in detailed information about how to sample different regions of the synthetic MRI images. With this, it becomes possible to generate arbitrary many randomly sampled and randomly augmented training pairs. Selecting and training a neural network model for high-resolution data brings its own set of challenges, such as dealing with memory constraints on today's GPUs. We will consider different strategies from different angles like patchwise or sliding window training, getting hold of NVidia GPUs larger than the 48GB available, or training on AMD APUs which have shared CPU/GPU memory. Once the training is done, we will verify the quality of the segmentation with different measures against the manual segmentations provided in the UltraCortex dataset.","title":"Overview"},{"location":"#getting-started","text":"Installation Contributing","title":"Getting Started"},{"location":"#features","text":"Scale label images for brain segmentation More features coming soon...","title":"Features"},{"location":"api/","text":"API Reference \u00b6 This section provides detailed API documentation for the Brain Segmentation project. brainseg \u00b6 The brainseg package contains modules for brain segmentation tasks. scale_label_image \u00b6 This module provides functionality for manipulating and resampling medical images in the NIfTI format using PyTorch and NumPy as primary computational backends. Options dataclass \u00b6 This program resamples a label image to a specified resolution using the NIfTI format. It supports optional Gaussian smoothing during the rescaling process. The user can provide input parameters, including the image file, output directory, desired resolution (in mm), and an optional smoothing sigma. The rescaled image is saved in the specified output directory. Source code in src/brainseg/scale_label_image.py @dataclass class Options: \"\"\" This program resamples a label image to a specified resolution using the NIfTI format. It supports optional Gaussian smoothing during the rescaling process. The user can provide input parameters, including the image file, output directory, desired resolution (in mm), and an optional smoothing sigma. The rescaled image is saved in the specified output directory. \"\"\" image_file: str \"\"\"Input label image for rescaling.\"\"\" output_dir: str \"\"\" Output directory where to store the resampled image. \"\"\" resolution: float \"\"\" Resolution in mm for the resampled label image. \"\"\" sigma: Optional[float] = None \"\"\" If not None, it must be a float value specifying the standard deviation of the Gaussian kernel to be used for smoothing the label image. \"\"\" image_file instance-attribute \u00b6 Input label image for rescaling. output_dir instance-attribute \u00b6 Output directory where to store the resampled image. resolution instance-attribute \u00b6 Resolution in mm for the resampled label image. sigma = None class-attribute instance-attribute \u00b6 If not None, it must be a float value specifying the standard deviation of the Gaussian kernel to be used for smoothing the label image. do_resample(nifti, resolution_out, device='cpu') \u00b6 Resamples a 3D NIfTI image to the desired resolution using nearest neighbor interpolation. Parameters: nifti ( Nifti1Image ) \u2013 The input 3D NIfTI image to be resampled. resolution_out ( ndarray ) \u2013 The desired output resolution, given as a 1D array containing three elements: (z-resolution, y-resolution, x-resolution). device ( str | device , default: 'cpu' ) \u2013 The device where computations are performed. Defaults to \"cpu\". Returns: Tensor \u2013 torch.Tensor: Resampled image data as a PyTorch tensor. Raises: RuntimeError \u2013 If the input image does not have exactly 3 dimensions. Source code in src/brainseg/scale_label_image.py def do_resample( nifti: nib.Nifti1Image, resolution_out: np.ndarray, device: str | torch.device = \"cpu\") -> torch.Tensor: \"\"\" Resamples a 3D NIfTI image to the desired resolution using nearest neighbor interpolation. Args: nifti (nib.Nifti1Image): The input 3D NIfTI image to be resampled. resolution_out (np.ndarray): The desired output resolution, given as a 1D array containing three elements: (z-resolution, y-resolution, x-resolution). device (str | torch.device): The device where computations are performed. Defaults to \"cpu\". Returns: torch.Tensor: Resampled image data as a PyTorch tensor. Raises: RuntimeError: If the input image does not have exactly 3 dimensions. \"\"\" header = nifti.header # noinspection PyUnresolvedReferences dim: tuple[int, int, int] = header.get_data_shape() if len(dim) != 3: raise RuntimeError(\"Image data does not have 3 dimensions\") # noinspection PyUnresolvedReferences resolution_in = header[\"pixdim\"][1:4] step_size: np.ndarray[np.float32] = resolution_out / resolution_in zs = torch.arange(0, dim[0], step_size[0]).to(dtype=torch.int, device=device) ys = torch.arange(0, dim[1], step_size[1]).to(dtype=torch.int, device=device) xs = torch.arange(0, dim[2], step_size[2]).to(dtype=torch.int, device=device) numpy_data = nifti.get_fdata() torch_data = torch.tensor(numpy_data, dtype=torch.int, device=device) data_resampled = torch_data[torch.meshgrid(zs, ys, xs, indexing=\"ij\")] return data_resampled gaussian_filter_fft(tensor, sigma) \u00b6 Filters a 3D tensor using a Gaussian kernel in the frequency domain. This function performs filtering of a 3-dimensional input tensor using a Gaussian kernel in the Fourier domain. The input tensor is transformed into the frequency domain via FFT, multiplied element-wise with the FFT of the Gaussian kernel, and then transformed back to the spatial domain using the inverse FFT. Parameters: tensor ( Tensor ) \u2013 A 3D PyTorch tensor to be filtered. The tensor must have three dimensions (ndim == 3). sigma ( float ) \u2013 The standard deviation of the Gaussian kernel used for filtering. Returns: Tensor \u2013 torch.Tensor: The filtered tensor in the spatial domain. Source code in src/brainseg/scale_label_image.py def gaussian_filter_fft(tensor: torch.Tensor, sigma: float) -> torch.Tensor: \"\"\" Filters a 3D tensor using a Gaussian kernel in the frequency domain. This function performs filtering of a 3-dimensional input tensor using a Gaussian kernel in the Fourier domain. The input tensor is transformed into the frequency domain via FFT, multiplied element-wise with the FFT of the Gaussian kernel, and then transformed back to the spatial domain using the inverse FFT. Args: tensor: A 3D PyTorch tensor to be filtered. The tensor must have three dimensions (ndim == 3). sigma: The standard deviation of the Gaussian kernel used for filtering. Returns: torch.Tensor: The filtered tensor in the spatial domain. \"\"\" assert tensor.ndim == 3, \"Input tensor must be 3D\" device = tensor.device shape = tensor.shape tensor_fft = torch.fft.fftn(tensor) kernel_fft = gaussian_kernel_3d_fft(shape, sigma, device=device) filtered_fft = tensor_fft * kernel_fft filtered = torch.fft.ifftn(filtered_fft).real return filtered gaussian_kernel_3d_fft(shape, sigma, device='cpu') \u00b6 Generates a 3D Gaussian kernel in the frequency domain using FFT. This function computes a 3D Gaussian kernel directly in the frequency domain. It takes the shape of the desired kernel, the standard deviation (sigma), and the device where the computation should be performed. The Gaussian kernel in the frequency domain is computed based on the squared Euclidean distance and the provided sigma. The analytical formula for the Gaussian kernel in the frequency domain can be found using the following Mathematica code for a multi-normal distribution and its Fourier transform:: covMatrix = {{sigma^2, 0, 0}, {0, sigma^2, 0}, {0, 0, sigma^2}}; distribution = PDF[MultinormalDistribution[covMatrix], {x, y, z}] kernel = FullSimplify[distribution, sigma > 0] Integrate[kernel, {x, -Infinity, Infinity}, {y, -Infinity, Infinity}, {z, -Infinity, Infinity}, Assumptions -> sigma > 0] FourierTransform[kernel, {x, y, z}, {X, Y, Z}, FourierParameters -> {0, -2*Pi}] Parameters: shape ( tuple [ int , int , int ] | Size ) \u2013 The shape of the 3D Gaussian kernel, defined as (nz, ny, nx), where nz, ny, and nx are the number of elements along the z, y, and x dimensions respectively. sigma ( float ) \u2013 The standard deviation of the Gaussian distribution. It determines the spread of the Gaussian kernel. device ( str | device , default: 'cpu' ) \u2013 The device where the computation should be performed, either 'cpu' or 'cuda'. The default is 'cpu'. Returns: Tensor \u2013 torch.Tensor: A 3D tensor representing the Gaussian kernel in the Tensor \u2013 frequency domain. The tensor has the same shape as the input shape . Source code in src/brainseg/scale_label_image.py def gaussian_kernel_3d_fft( shape: tuple[int, int, int] | torch.Size, sigma: float, device: str | torch.device = 'cpu') -> torch.Tensor: \"\"\" Generates a 3D Gaussian kernel in the frequency domain using FFT. This function computes a 3D Gaussian kernel directly in the frequency domain. It takes the shape of the desired kernel, the standard deviation (sigma), and the device where the computation should be performed. The Gaussian kernel in the frequency domain is computed based on the squared Euclidean distance and the provided sigma. The analytical formula for the Gaussian kernel in the frequency domain can be found using the following Mathematica code for a multi-normal distribution and its Fourier transform:: covMatrix = {{sigma^2, 0, 0}, {0, sigma^2, 0}, {0, 0, sigma^2}}; distribution = PDF[MultinormalDistribution[covMatrix], {x, y, z}] kernel = FullSimplify[distribution, sigma > 0] Integrate[kernel, {x, -Infinity, Infinity}, {y, -Infinity, Infinity}, {z, -Infinity, Infinity}, Assumptions -> sigma > 0] FourierTransform[kernel, {x, y, z}, {X, Y, Z}, FourierParameters -> {0, -2*Pi}] Args: shape: The shape of the 3D Gaussian kernel, defined as (nz, ny, nx), where nz, ny, and nx are the number of elements along the z, y, and x dimensions respectively. sigma: The standard deviation of the Gaussian distribution. It determines the spread of the Gaussian kernel. device: The device where the computation should be performed, either 'cpu' or 'cuda'. The default is 'cpu'. Returns: torch.Tensor: A 3D tensor representing the Gaussian kernel in the frequency domain. The tensor has the same shape as the input `shape`. \"\"\" if not sigma > 0.0: raise ValueError(f\"Sigma must be positive, got {sigma}\") nz, ny, nx = shape z = torch.fft.fftfreq(nz, device=device).view(nz, 1, 1) y = torch.fft.fftfreq(ny, device=device).view(1, ny, 1) x = torch.fft.fftfreq(nx, device=device).view(1, 1, nx) squared_dist = x ** 2 + y ** 2 + z ** 2 kernel_fft = torch.exp(-2.0 * torch.pi**2 * squared_dist * sigma ** 2) return kernel_fft main() \u00b6 Main entry point for the program. Source code in src/brainseg/scale_label_image.py def main(): \"\"\" Main entry point for the program. \"\"\" parser = ArgumentParser() # noinspection PyTypeChecker parser.add_arguments(Options, \"options\") args = parser.parse_args() options: Options = args.options if isinstance(options.output_dir, str) and os.path.isdir(options.output_dir): logger.debug(f\"Using output directory: '{options.output_dir}'\") else: logger.error(f\"Output directory does not exist: '{options.output_dir}'\") sys.exit(1) if isinstance(options.image_file, str) and os.path.isfile(options.image_file): logger.debug(f\"Using label image: '{options.image_file}'\") else: logger.error(f\"Provided image is not a regular file: '{options.image_file}'\") sys.exit(1) resolution = options.resolution output_dir = options.output_dir image_file = options.image_file nifti = nib.load(image_file) if not isinstance(nifti, nib.Nifti1Image): logger.error(f\"Image {image_file} is not a Nifti1 image\") sys.exit(1) result = resample_label_image(nifti, resolution) if not isinstance(result, nib.Nifti1Image): logger.error(\"Unable to rescale image.\") file_base = os.path.basename(image_file) output_file = os.path.join(output_dir, file_base) nib.save(result, output_file) resample_label_image(nifti, resolution_out, sigma=None, device='cpu') \u00b6 Resample a label image to a new resolution. Note: This is intended for internal use only to upsample label images that are in a too low resolution. Parameters: nifti ( Nifti1Image ) \u2013 The input label-image represented as a nib.Nifti1Image object. resolution_out ( Union [ float , List [ float ]] ) \u2013 The desired output resolution. It can be a single float value or a list of three float values representing the desired voxel size in mm in x, y, and z directions respectively. sigma ( Optional [ float ] , default: None ) \u2013 If not None, it must be a float value specifying the standard deviation of the Gaussian kernel to be used for smoothing the label image. device ( str | device , default: 'cpu' ) \u2013 The device where computations are performed. Defaults to \"cpu\". Source code in src/brainseg/scale_label_image.py def resample_label_image(nifti: nib.Nifti1Image, resolution_out: Union[float, List[float]], sigma: Optional[float] = None, device: str | torch.device = \"cpu\") -> nib.Nifti1Image: \"\"\" Resample a label image to a new resolution. Note: This is intended for internal use only to upsample label images that are in a too low resolution. Args: nifti: The input label-image represented as a nib.Nifti1Image object. resolution_out: The desired output resolution. It can be a single float value or a list of three float values representing the desired voxel size in mm in x, y, and z directions respectively. sigma: If not None, it must be a float value specifying the standard deviation of the Gaussian kernel to be used for smoothing the label image. device: The device where computations are performed. Defaults to \"cpu\". Returns: The resampled label-image represented as a nib.Nifti1Image object. \"\"\" if isinstance(resolution_out, float): resolution_out = np.array([resolution_out] * 3) else: resolution_out = np.array(resolution_out) data_resampled = do_resample(nifti, resolution_out, device=device) if sigma is not None: data_resampled = smooth_label_image(data_resampled, sigma) header = nifti.header # noinspection PyUnresolvedReferences header.set_zooms(resolution_out) # noinspection PyTypeChecker return nib.Nifti1Image(data_resampled.numpy(force=True).astype(np.uint16), nifti.affine, header) smooth_label_image(data, sigma) \u00b6 Smooth a label image using a Gaussian kernel of size sigma. This function applies Gaussian smoothing to a label image tensor and assigns each label the maximum probability using a Gaussian Kernel. It processes each unique label separately, applies 3D FFT-based Gaussian smoothing, and updates the resulting tensor with the labels based on maximum probability. Parameters: data ( Tensor ) \u2013 A tensor containing the label image to be smoothed. sigma ( float ) \u2013 Standard deviation for the Gaussian kernel used for smoothing. Returns: Tensor \u2013 torch.Tensor: A tensor representing the smoothed label image. Source code in src/brainseg/scale_label_image.py def smooth_label_image(data: torch.Tensor, sigma: float) -> torch.Tensor: \"\"\" Smooth a label image using a Gaussian kernel of size sigma. This function applies Gaussian smoothing to a label image tensor and assigns each label the maximum probability using a Gaussian Kernel. It processes each unique label separately, applies 3D FFT-based Gaussian smoothing, and updates the resulting tensor with the labels based on maximum probability. Args: data: A tensor containing the label image to be smoothed. sigma: Standard deviation for the Gaussian kernel used for smoothing. Returns: torch.Tensor: A tensor representing the smoothed label image. \"\"\" labels = torch.unique(data) kernel_fft = gaussian_kernel_3d_fft(data.shape, sigma, device=data.device) max_probabilities = torch.zeros_like(data, dtype=torch.float32) result = torch.zeros_like(data) logger.info(f\"Smoothing {len(labels)} labels\") tqd = tqdm.tqdm(labels, desc=\"Smoothing label\") for label in tqd: tqd.set_postfix({\"label\": label}) filtered = torch.fft.ifftn(torch.fft.fftn((data == label).float()) * kernel_fft).real mask2 = filtered > max_probabilities result[mask2] = label max_probabilities[mask2] = filtered[mask2] return result","title":"API Reference"},{"location":"api/#api-reference","text":"This section provides detailed API documentation for the Brain Segmentation project.","title":"API Reference"},{"location":"api/#brainseg","text":"The brainseg package contains modules for brain segmentation tasks.","title":"brainseg"},{"location":"api/#scale_label_image","text":"This module provides functionality for manipulating and resampling medical images in the NIfTI format using PyTorch and NumPy as primary computational backends.","title":"scale_label_image"},{"location":"api/#brainseg.scale_label_image.Options","text":"This program resamples a label image to a specified resolution using the NIfTI format. It supports optional Gaussian smoothing during the rescaling process. The user can provide input parameters, including the image file, output directory, desired resolution (in mm), and an optional smoothing sigma. The rescaled image is saved in the specified output directory. Source code in src/brainseg/scale_label_image.py @dataclass class Options: \"\"\" This program resamples a label image to a specified resolution using the NIfTI format. It supports optional Gaussian smoothing during the rescaling process. The user can provide input parameters, including the image file, output directory, desired resolution (in mm), and an optional smoothing sigma. The rescaled image is saved in the specified output directory. \"\"\" image_file: str \"\"\"Input label image for rescaling.\"\"\" output_dir: str \"\"\" Output directory where to store the resampled image. \"\"\" resolution: float \"\"\" Resolution in mm for the resampled label image. \"\"\" sigma: Optional[float] = None \"\"\" If not None, it must be a float value specifying the standard deviation of the Gaussian kernel to be used for smoothing the label image. \"\"\"","title":"Options"},{"location":"api/#brainseg.scale_label_image.Options.image_file","text":"Input label image for rescaling.","title":"image_file"},{"location":"api/#brainseg.scale_label_image.Options.output_dir","text":"Output directory where to store the resampled image.","title":"output_dir"},{"location":"api/#brainseg.scale_label_image.Options.resolution","text":"Resolution in mm for the resampled label image.","title":"resolution"},{"location":"api/#brainseg.scale_label_image.Options.sigma","text":"If not None, it must be a float value specifying the standard deviation of the Gaussian kernel to be used for smoothing the label image.","title":"sigma"},{"location":"api/#brainseg.scale_label_image.do_resample","text":"Resamples a 3D NIfTI image to the desired resolution using nearest neighbor interpolation. Parameters: nifti ( Nifti1Image ) \u2013 The input 3D NIfTI image to be resampled. resolution_out ( ndarray ) \u2013 The desired output resolution, given as a 1D array containing three elements: (z-resolution, y-resolution, x-resolution). device ( str | device , default: 'cpu' ) \u2013 The device where computations are performed. Defaults to \"cpu\". Returns: Tensor \u2013 torch.Tensor: Resampled image data as a PyTorch tensor. Raises: RuntimeError \u2013 If the input image does not have exactly 3 dimensions. Source code in src/brainseg/scale_label_image.py def do_resample( nifti: nib.Nifti1Image, resolution_out: np.ndarray, device: str | torch.device = \"cpu\") -> torch.Tensor: \"\"\" Resamples a 3D NIfTI image to the desired resolution using nearest neighbor interpolation. Args: nifti (nib.Nifti1Image): The input 3D NIfTI image to be resampled. resolution_out (np.ndarray): The desired output resolution, given as a 1D array containing three elements: (z-resolution, y-resolution, x-resolution). device (str | torch.device): The device where computations are performed. Defaults to \"cpu\". Returns: torch.Tensor: Resampled image data as a PyTorch tensor. Raises: RuntimeError: If the input image does not have exactly 3 dimensions. \"\"\" header = nifti.header # noinspection PyUnresolvedReferences dim: tuple[int, int, int] = header.get_data_shape() if len(dim) != 3: raise RuntimeError(\"Image data does not have 3 dimensions\") # noinspection PyUnresolvedReferences resolution_in = header[\"pixdim\"][1:4] step_size: np.ndarray[np.float32] = resolution_out / resolution_in zs = torch.arange(0, dim[0], step_size[0]).to(dtype=torch.int, device=device) ys = torch.arange(0, dim[1], step_size[1]).to(dtype=torch.int, device=device) xs = torch.arange(0, dim[2], step_size[2]).to(dtype=torch.int, device=device) numpy_data = nifti.get_fdata() torch_data = torch.tensor(numpy_data, dtype=torch.int, device=device) data_resampled = torch_data[torch.meshgrid(zs, ys, xs, indexing=\"ij\")] return data_resampled","title":"do_resample"},{"location":"api/#brainseg.scale_label_image.gaussian_filter_fft","text":"Filters a 3D tensor using a Gaussian kernel in the frequency domain. This function performs filtering of a 3-dimensional input tensor using a Gaussian kernel in the Fourier domain. The input tensor is transformed into the frequency domain via FFT, multiplied element-wise with the FFT of the Gaussian kernel, and then transformed back to the spatial domain using the inverse FFT. Parameters: tensor ( Tensor ) \u2013 A 3D PyTorch tensor to be filtered. The tensor must have three dimensions (ndim == 3). sigma ( float ) \u2013 The standard deviation of the Gaussian kernel used for filtering. Returns: Tensor \u2013 torch.Tensor: The filtered tensor in the spatial domain. Source code in src/brainseg/scale_label_image.py def gaussian_filter_fft(tensor: torch.Tensor, sigma: float) -> torch.Tensor: \"\"\" Filters a 3D tensor using a Gaussian kernel in the frequency domain. This function performs filtering of a 3-dimensional input tensor using a Gaussian kernel in the Fourier domain. The input tensor is transformed into the frequency domain via FFT, multiplied element-wise with the FFT of the Gaussian kernel, and then transformed back to the spatial domain using the inverse FFT. Args: tensor: A 3D PyTorch tensor to be filtered. The tensor must have three dimensions (ndim == 3). sigma: The standard deviation of the Gaussian kernel used for filtering. Returns: torch.Tensor: The filtered tensor in the spatial domain. \"\"\" assert tensor.ndim == 3, \"Input tensor must be 3D\" device = tensor.device shape = tensor.shape tensor_fft = torch.fft.fftn(tensor) kernel_fft = gaussian_kernel_3d_fft(shape, sigma, device=device) filtered_fft = tensor_fft * kernel_fft filtered = torch.fft.ifftn(filtered_fft).real return filtered","title":"gaussian_filter_fft"},{"location":"api/#brainseg.scale_label_image.gaussian_kernel_3d_fft","text":"Generates a 3D Gaussian kernel in the frequency domain using FFT. This function computes a 3D Gaussian kernel directly in the frequency domain. It takes the shape of the desired kernel, the standard deviation (sigma), and the device where the computation should be performed. The Gaussian kernel in the frequency domain is computed based on the squared Euclidean distance and the provided sigma. The analytical formula for the Gaussian kernel in the frequency domain can be found using the following Mathematica code for a multi-normal distribution and its Fourier transform:: covMatrix = {{sigma^2, 0, 0}, {0, sigma^2, 0}, {0, 0, sigma^2}}; distribution = PDF[MultinormalDistribution[covMatrix], {x, y, z}] kernel = FullSimplify[distribution, sigma > 0] Integrate[kernel, {x, -Infinity, Infinity}, {y, -Infinity, Infinity}, {z, -Infinity, Infinity}, Assumptions -> sigma > 0] FourierTransform[kernel, {x, y, z}, {X, Y, Z}, FourierParameters -> {0, -2*Pi}] Parameters: shape ( tuple [ int , int , int ] | Size ) \u2013 The shape of the 3D Gaussian kernel, defined as (nz, ny, nx), where nz, ny, and nx are the number of elements along the z, y, and x dimensions respectively. sigma ( float ) \u2013 The standard deviation of the Gaussian distribution. It determines the spread of the Gaussian kernel. device ( str | device , default: 'cpu' ) \u2013 The device where the computation should be performed, either 'cpu' or 'cuda'. The default is 'cpu'. Returns: Tensor \u2013 torch.Tensor: A 3D tensor representing the Gaussian kernel in the Tensor \u2013 frequency domain. The tensor has the same shape as the input shape . Source code in src/brainseg/scale_label_image.py def gaussian_kernel_3d_fft( shape: tuple[int, int, int] | torch.Size, sigma: float, device: str | torch.device = 'cpu') -> torch.Tensor: \"\"\" Generates a 3D Gaussian kernel in the frequency domain using FFT. This function computes a 3D Gaussian kernel directly in the frequency domain. It takes the shape of the desired kernel, the standard deviation (sigma), and the device where the computation should be performed. The Gaussian kernel in the frequency domain is computed based on the squared Euclidean distance and the provided sigma. The analytical formula for the Gaussian kernel in the frequency domain can be found using the following Mathematica code for a multi-normal distribution and its Fourier transform:: covMatrix = {{sigma^2, 0, 0}, {0, sigma^2, 0}, {0, 0, sigma^2}}; distribution = PDF[MultinormalDistribution[covMatrix], {x, y, z}] kernel = FullSimplify[distribution, sigma > 0] Integrate[kernel, {x, -Infinity, Infinity}, {y, -Infinity, Infinity}, {z, -Infinity, Infinity}, Assumptions -> sigma > 0] FourierTransform[kernel, {x, y, z}, {X, Y, Z}, FourierParameters -> {0, -2*Pi}] Args: shape: The shape of the 3D Gaussian kernel, defined as (nz, ny, nx), where nz, ny, and nx are the number of elements along the z, y, and x dimensions respectively. sigma: The standard deviation of the Gaussian distribution. It determines the spread of the Gaussian kernel. device: The device where the computation should be performed, either 'cpu' or 'cuda'. The default is 'cpu'. Returns: torch.Tensor: A 3D tensor representing the Gaussian kernel in the frequency domain. The tensor has the same shape as the input `shape`. \"\"\" if not sigma > 0.0: raise ValueError(f\"Sigma must be positive, got {sigma}\") nz, ny, nx = shape z = torch.fft.fftfreq(nz, device=device).view(nz, 1, 1) y = torch.fft.fftfreq(ny, device=device).view(1, ny, 1) x = torch.fft.fftfreq(nx, device=device).view(1, 1, nx) squared_dist = x ** 2 + y ** 2 + z ** 2 kernel_fft = torch.exp(-2.0 * torch.pi**2 * squared_dist * sigma ** 2) return kernel_fft","title":"gaussian_kernel_3d_fft"},{"location":"api/#brainseg.scale_label_image.main","text":"Main entry point for the program. Source code in src/brainseg/scale_label_image.py def main(): \"\"\" Main entry point for the program. \"\"\" parser = ArgumentParser() # noinspection PyTypeChecker parser.add_arguments(Options, \"options\") args = parser.parse_args() options: Options = args.options if isinstance(options.output_dir, str) and os.path.isdir(options.output_dir): logger.debug(f\"Using output directory: '{options.output_dir}'\") else: logger.error(f\"Output directory does not exist: '{options.output_dir}'\") sys.exit(1) if isinstance(options.image_file, str) and os.path.isfile(options.image_file): logger.debug(f\"Using label image: '{options.image_file}'\") else: logger.error(f\"Provided image is not a regular file: '{options.image_file}'\") sys.exit(1) resolution = options.resolution output_dir = options.output_dir image_file = options.image_file nifti = nib.load(image_file) if not isinstance(nifti, nib.Nifti1Image): logger.error(f\"Image {image_file} is not a Nifti1 image\") sys.exit(1) result = resample_label_image(nifti, resolution) if not isinstance(result, nib.Nifti1Image): logger.error(\"Unable to rescale image.\") file_base = os.path.basename(image_file) output_file = os.path.join(output_dir, file_base) nib.save(result, output_file)","title":"main"},{"location":"api/#brainseg.scale_label_image.resample_label_image","text":"Resample a label image to a new resolution. Note: This is intended for internal use only to upsample label images that are in a too low resolution. Parameters: nifti ( Nifti1Image ) \u2013 The input label-image represented as a nib.Nifti1Image object. resolution_out ( Union [ float , List [ float ]] ) \u2013 The desired output resolution. It can be a single float value or a list of three float values representing the desired voxel size in mm in x, y, and z directions respectively. sigma ( Optional [ float ] , default: None ) \u2013 If not None, it must be a float value specifying the standard deviation of the Gaussian kernel to be used for smoothing the label image. device ( str | device , default: 'cpu' ) \u2013 The device where computations are performed. Defaults to \"cpu\". Source code in src/brainseg/scale_label_image.py def resample_label_image(nifti: nib.Nifti1Image, resolution_out: Union[float, List[float]], sigma: Optional[float] = None, device: str | torch.device = \"cpu\") -> nib.Nifti1Image: \"\"\" Resample a label image to a new resolution. Note: This is intended for internal use only to upsample label images that are in a too low resolution. Args: nifti: The input label-image represented as a nib.Nifti1Image object. resolution_out: The desired output resolution. It can be a single float value or a list of three float values representing the desired voxel size in mm in x, y, and z directions respectively. sigma: If not None, it must be a float value specifying the standard deviation of the Gaussian kernel to be used for smoothing the label image. device: The device where computations are performed. Defaults to \"cpu\". Returns: The resampled label-image represented as a nib.Nifti1Image object. \"\"\" if isinstance(resolution_out, float): resolution_out = np.array([resolution_out] * 3) else: resolution_out = np.array(resolution_out) data_resampled = do_resample(nifti, resolution_out, device=device) if sigma is not None: data_resampled = smooth_label_image(data_resampled, sigma) header = nifti.header # noinspection PyUnresolvedReferences header.set_zooms(resolution_out) # noinspection PyTypeChecker return nib.Nifti1Image(data_resampled.numpy(force=True).astype(np.uint16), nifti.affine, header)","title":"resample_label_image"},{"location":"api/#brainseg.scale_label_image.smooth_label_image","text":"Smooth a label image using a Gaussian kernel of size sigma. This function applies Gaussian smoothing to a label image tensor and assigns each label the maximum probability using a Gaussian Kernel. It processes each unique label separately, applies 3D FFT-based Gaussian smoothing, and updates the resulting tensor with the labels based on maximum probability. Parameters: data ( Tensor ) \u2013 A tensor containing the label image to be smoothed. sigma ( float ) \u2013 Standard deviation for the Gaussian kernel used for smoothing. Returns: Tensor \u2013 torch.Tensor: A tensor representing the smoothed label image. Source code in src/brainseg/scale_label_image.py def smooth_label_image(data: torch.Tensor, sigma: float) -> torch.Tensor: \"\"\" Smooth a label image using a Gaussian kernel of size sigma. This function applies Gaussian smoothing to a label image tensor and assigns each label the maximum probability using a Gaussian Kernel. It processes each unique label separately, applies 3D FFT-based Gaussian smoothing, and updates the resulting tensor with the labels based on maximum probability. Args: data: A tensor containing the label image to be smoothed. sigma: Standard deviation for the Gaussian kernel used for smoothing. Returns: torch.Tensor: A tensor representing the smoothed label image. \"\"\" labels = torch.unique(data) kernel_fft = gaussian_kernel_3d_fft(data.shape, sigma, device=data.device) max_probabilities = torch.zeros_like(data, dtype=torch.float32) result = torch.zeros_like(data) logger.info(f\"Smoothing {len(labels)} labels\") tqd = tqdm.tqdm(labels, desc=\"Smoothing label\") for label in tqd: tqd.set_postfix({\"label\": label}) filtered = torch.fft.ifftn(torch.fft.fftn((data == label).float()) * kernel_fft).real mask2 = filtered > max_probabilities result[mask2] = label max_probabilities[mask2] = filtered[mask2] return result","title":"smooth_label_image"},{"location":"installation/","text":"Installation \u00b6 This guide will help you set up the Brain Segmentation project on your local machine. Prerequisites \u00b6 Python 3.9 or higher Miniforge to install Conda ( Download ) Installation Steps \u00b6 1. Clone the Repository \u00b6 git clone https://github.com/yourusername/BrainSegmentation.git cd BrainSegmentation 2. Set Up the Environment \u00b6 We provide a Conda environment file to make setup easier: conda env create -f python_setup/environment.yml conda activate brainseg","title":"Installation"},{"location":"installation/#installation","text":"This guide will help you set up the Brain Segmentation project on your local machine.","title":"Installation"},{"location":"installation/#prerequisites","text":"Python 3.9 or higher Miniforge to install Conda ( Download )","title":"Prerequisites"},{"location":"installation/#installation-steps","text":"","title":"Installation Steps"},{"location":"installation/#1-clone-the-repository","text":"git clone https://github.com/yourusername/BrainSegmentation.git cd BrainSegmentation","title":"1. Clone the Repository"},{"location":"installation/#2-set-up-the-environment","text":"We provide a Conda environment file to make setup easier: conda env create -f python_setup/environment.yml conda activate brainseg","title":"2. Set Up the Environment"},{"location":"cluster/apptainer/","text":"Building an Apptainer on MPCDF \u00b6 Apptainer (formerly known as Singularity) is a container platform specifically designed for scientific computing and High-Performance Computing (HPC) environments. Unlike traditional containers, Apptainer is built with security and compatibility with research computing systems in mind. It allows scientists and researchers to package their entire computational environment, including the operating system, software, libraries, and data, into a single, portable container that can run consistently across different computing systems. Building an Apptainer can be tricky, sometimes painful. However, in general you can think of it as setting up a new machine. The recipy of setting up your machine is stored in a .def text file which defines step-by-step what you will install and set up in your Apptainer. The .def file has several %name sections that define different stages and the behavior when building and running your Apptainer. You decide which operating system you want to use. In scientific computing, we usually use Linux. That is the base container your Apptainer will build on. What you will get is a fresh Linux installation. Now you can install the software you need, and you can use the distribution's package manager. So on an Ubuntu, you use apt get . After your system is set up, you can download and install additional things like Python environments or code from GitHub repositories. In the end, your goal is to have a fully self-contained Apptainer that is able to run the computations you need. The container itself is just a .sif file and starting or running your new machine is as easy as calling apptainer with your .sif file. Building the BrainSegmentation Apptainer \u00b6 To build a container that contains conda, the BrainSegmentation code, SynthSeg, CHARM Segmentation, and different conda environments for each, you can use the following on the MPCDF cluster: git clone https://github.com/halirutan/BrainSegmentation.git module purge module load apptainer apptainer build mpcdf_raven.sif BrainSegmentation/apptainer/mpcdf_raven.def The apptainer directory also contains an example SLURM script with an accompanying shell script to run a test. However, you will need to adapt the paths in the scripts to your needs.","title":"Apptainer Build"},{"location":"cluster/apptainer/#building-an-apptainer-on-mpcdf","text":"Apptainer (formerly known as Singularity) is a container platform specifically designed for scientific computing and High-Performance Computing (HPC) environments. Unlike traditional containers, Apptainer is built with security and compatibility with research computing systems in mind. It allows scientists and researchers to package their entire computational environment, including the operating system, software, libraries, and data, into a single, portable container that can run consistently across different computing systems. Building an Apptainer can be tricky, sometimes painful. However, in general you can think of it as setting up a new machine. The recipy of setting up your machine is stored in a .def text file which defines step-by-step what you will install and set up in your Apptainer. The .def file has several %name sections that define different stages and the behavior when building and running your Apptainer. You decide which operating system you want to use. In scientific computing, we usually use Linux. That is the base container your Apptainer will build on. What you will get is a fresh Linux installation. Now you can install the software you need, and you can use the distribution's package manager. So on an Ubuntu, you use apt get . After your system is set up, you can download and install additional things like Python environments or code from GitHub repositories. In the end, your goal is to have a fully self-contained Apptainer that is able to run the computations you need. The container itself is just a .sif file and starting or running your new machine is as easy as calling apptainer with your .sif file.","title":"Building an Apptainer on MPCDF"},{"location":"cluster/apptainer/#building-the-brainsegmentation-apptainer","text":"To build a container that contains conda, the BrainSegmentation code, SynthSeg, CHARM Segmentation, and different conda environments for each, you can use the following on the MPCDF cluster: git clone https://github.com/halirutan/BrainSegmentation.git module purge module load apptainer apptainer build mpcdf_raven.sif BrainSegmentation/apptainer/mpcdf_raven.def The apptainer directory also contains an example SLURM script with an accompanying shell script to run a test. However, you will need to adapt the paths in the scripts to your needs.","title":"Building the BrainSegmentation Apptainer"},{"location":"cluster/info/","text":"MPCDF High-Performance Cluster \u00b6 The Max Planck Computing and Data Facility (MPCDF) provides high-performance computing (HPC) resources to support computational and data-driven research within the Max Planck Society. These resources are designed for large-scale simulations, complex data analyses, and other intensive scientific workloads. MPCDF operates several powerful computing systems. The flagship system, Raven , includes over 1,500 compute nodes based on Intel Xeon processors, many of which are equipped with large memory and high-speed interconnects. It also features GPU-accelerated nodes with NVIDIA A100 GPUs, ideal for machine learning and parallel computations. Another system, Viper , offers AMD EPYC-based nodes with high core counts and large memory configurations, optimized for both general-purpose and memory-intensive applications. Users access the systems remotely via secure SSH connections. Work is typically done through a shared Linux environment with user-configurable software modules. Job scheduling is managed using the Slurm workload manager, allowing users to submit computational tasks that are distributed across the available resources. The systems support batch processing as well as interactive sessions for development and debugging. Important Links: Request Access Raven Documentation Viper CPU Documentation Viper GPU Documentation FAQ \u00b6 In general, please check the MPCDF FAQ . Here are some specific questions that we found useful. What software is available on the cluster? \u00b6 MPCDF uses as module system that allows you to load software packages on demand. Please read here for more information . Do I need to type my password every time I'm logging in with SSH \u00b6 No. On Linux and macOS, you can use an SSH ControlMaster configuration that allows you to reuse your SSH connection for several hours. This setup also lets you directly log into the raven and viper nodes without having to go through the gateway machines. Read here Can I test computations interactively? \u00b6 Yes, to some degree. The login nodes like raven-i or viper-i are suitable for interactive use, but they are not suitable for running computations because they are too weak, are used by too many people, and don't contain GPUs. However, you can specify dedicated [SLURM partitions] and srun to get a real interactive session for a limited amount of time. Note Different partitions have different restrictions for the resources. E.g., an interactive GPU session on Raven cannot exceed 15 minutes. Here are the commands for various partitions but also read further for info for Raven , Viper GPU , and Viper CPU : Raven interactive CPU session for 30 minutes: srun --verbose -p interactive --time=00:30:00 --pty bash Raven interactive GPU session with an A100 GPU for 15 minutes: srun --verbose --gres=gpu:a100:1 -p gpudev --time=15 --pty bash How do I find out about the properties of SLURM partitions? \u00b6 All SLURM commands start with an s . First, you can list all available partitions in a nicely formatted table like this: sinfo -o \"%20P %5D %10c %10m %25f %10G\" On Raven, the output will look like the following, and it will show you the partition names and their properties: PARTITION NODES CPUS MEMORY AVAIL_FEATURES GRES interactive* 2 144 512000 login,icelake (null) general 144 144 240000 icelake,fhi,cpu (null) general 1378 144 240000 icelake,cpu (null) general 157 144 500000 icelake,gpu gpu:a100:4 general 32 144 500000 icelake,gpu-bw gpu:a100:4 general 4 144 2048000 icelake,hugemem (null) general 64 144 500000 icelake,largemem (null) small 144 144 240000 icelake,fhi,cpu (null) small 1378 144 240000 icelake,cpu (null) gpu 157 144 500000 icelake,gpu gpu:a100:4 gpu 32 144 500000 icelake,gpu-bw gpu:a100:4 gpu1 157 144 500000 icelake,gpu gpu:a100:4 gpu1 32 144 500000 icelake,gpu-bw gpu:a100:4 rvs 2 144 240000 icelake,cpu (null) rvs 2 144 500000 icelake,gpu gpu:a100:4 gpudev 1 144 500000 icelake,gpu gpu:a100:4 Now, to inspect a specific partition in detail, you can use the scontrol command. Since we were looking for the partition gpudev before, let's use this as an example: scontrol show partition gpudev PartitionName=gpudev AllowGroups=ALL AllowAccounts=ALL AllowQos=ALL AllocNodes=ALL Default=NO CpuBind=cores QoS=N/A DefaultTime=NONE DisableRootJobs=NO ExclusiveUser=NO GraceTime=0 Hidden=NO MaxNodes=1 MaxTime=00:15:00 MinNodes=1 LLN=NO MaxCPUsPerNode=UNLIMITED MaxCPUsPerSocket=UNLIMITED NodeSets=dev Nodes=ravg1002 PriorityJobFactor=1 PriorityTier=1 RootOnly=NO ReqResv=NO OverSubscribe=NO OverTimeLimit=NONE PreemptMode=OFF State=UP TotalCPUs=144 TotalNodes=1 SelectTypeParameters=NONE JobDefaults=DefCpuPerGPU=36 DefMemPerNode=125000 MaxMemPerNode=UNLIMITED TRES=cpu=144,mem=500000M,node=1,billing=144,gres/gpu=4,gres/gpu:a100=4 Note the MaxTime=00:15:00 property. Can I share/access data of other users? \u00b6 Yes. We usually share project data in /ptmp/myuser using setfacl and getfacl . Please read here for more information . SLURM: What's the point in specifying memory, cores, and time requirements? \u00b6 Short answer: The less you specify, the quicker your job gets scheduled. On a system like the HPC, many users compete for recourses. The SLURM scheduler tries to find a spot for your job that is as close to your requirements as possible. If it can fit your job into a spot, because, e.g., another user doesn't need a whole node and your job only needs a bit of computational power, then it will do so. On the other hand, if you specify a lot of memory or a lot of cores, then the scheduler might need to put you at the back of the queue. Therefore, always specify the least you need. What's the difference between Raven and Viper? \u00b6 One answer is: Raven is older and Viper is newer. The other answer is: Raven has Intel CPUs and NVidia A100 GPUs, while Viper has AMD CPUs and GPUs. From a practical point of view, the Viper system is interesting because the AMD architecture offers shared memory between CPU and GPU cores (AMD calls these things APU), and it has 128GB per node. When you have a GPU accelerated program and you are fine with 40GB NVidia A100 cards, then use Raven. Otherwise, it is worth trying Viper.","title":"General Information"},{"location":"cluster/info/#mpcdf-high-performance-cluster","text":"The Max Planck Computing and Data Facility (MPCDF) provides high-performance computing (HPC) resources to support computational and data-driven research within the Max Planck Society. These resources are designed for large-scale simulations, complex data analyses, and other intensive scientific workloads. MPCDF operates several powerful computing systems. The flagship system, Raven , includes over 1,500 compute nodes based on Intel Xeon processors, many of which are equipped with large memory and high-speed interconnects. It also features GPU-accelerated nodes with NVIDIA A100 GPUs, ideal for machine learning and parallel computations. Another system, Viper , offers AMD EPYC-based nodes with high core counts and large memory configurations, optimized for both general-purpose and memory-intensive applications. Users access the systems remotely via secure SSH connections. Work is typically done through a shared Linux environment with user-configurable software modules. Job scheduling is managed using the Slurm workload manager, allowing users to submit computational tasks that are distributed across the available resources. The systems support batch processing as well as interactive sessions for development and debugging. Important Links: Request Access Raven Documentation Viper CPU Documentation Viper GPU Documentation","title":"MPCDF High-Performance Cluster"},{"location":"cluster/info/#faq","text":"In general, please check the MPCDF FAQ . Here are some specific questions that we found useful.","title":"FAQ"},{"location":"cluster/info/#what-software-is-available-on-the-cluster","text":"MPCDF uses as module system that allows you to load software packages on demand. Please read here for more information .","title":"What software is available on the cluster?"},{"location":"cluster/info/#do-i-need-to-type-my-password-every-time-im-logging-in-with-ssh","text":"No. On Linux and macOS, you can use an SSH ControlMaster configuration that allows you to reuse your SSH connection for several hours. This setup also lets you directly log into the raven and viper nodes without having to go through the gateway machines. Read here","title":"Do I need to type my password every time I'm logging in with SSH"},{"location":"cluster/info/#can-i-test-computations-interactively","text":"Yes, to some degree. The login nodes like raven-i or viper-i are suitable for interactive use, but they are not suitable for running computations because they are too weak, are used by too many people, and don't contain GPUs. However, you can specify dedicated [SLURM partitions] and srun to get a real interactive session for a limited amount of time. Note Different partitions have different restrictions for the resources. E.g., an interactive GPU session on Raven cannot exceed 15 minutes. Here are the commands for various partitions but also read further for info for Raven , Viper GPU , and Viper CPU : Raven interactive CPU session for 30 minutes: srun --verbose -p interactive --time=00:30:00 --pty bash Raven interactive GPU session with an A100 GPU for 15 minutes: srun --verbose --gres=gpu:a100:1 -p gpudev --time=15 --pty bash","title":"Can I test computations interactively?"},{"location":"cluster/info/#how-do-i-find-out-about-the-properties-of-slurm-partitions","text":"All SLURM commands start with an s . First, you can list all available partitions in a nicely formatted table like this: sinfo -o \"%20P %5D %10c %10m %25f %10G\" On Raven, the output will look like the following, and it will show you the partition names and their properties: PARTITION NODES CPUS MEMORY AVAIL_FEATURES GRES interactive* 2 144 512000 login,icelake (null) general 144 144 240000 icelake,fhi,cpu (null) general 1378 144 240000 icelake,cpu (null) general 157 144 500000 icelake,gpu gpu:a100:4 general 32 144 500000 icelake,gpu-bw gpu:a100:4 general 4 144 2048000 icelake,hugemem (null) general 64 144 500000 icelake,largemem (null) small 144 144 240000 icelake,fhi,cpu (null) small 1378 144 240000 icelake,cpu (null) gpu 157 144 500000 icelake,gpu gpu:a100:4 gpu 32 144 500000 icelake,gpu-bw gpu:a100:4 gpu1 157 144 500000 icelake,gpu gpu:a100:4 gpu1 32 144 500000 icelake,gpu-bw gpu:a100:4 rvs 2 144 240000 icelake,cpu (null) rvs 2 144 500000 icelake,gpu gpu:a100:4 gpudev 1 144 500000 icelake,gpu gpu:a100:4 Now, to inspect a specific partition in detail, you can use the scontrol command. Since we were looking for the partition gpudev before, let's use this as an example: scontrol show partition gpudev PartitionName=gpudev AllowGroups=ALL AllowAccounts=ALL AllowQos=ALL AllocNodes=ALL Default=NO CpuBind=cores QoS=N/A DefaultTime=NONE DisableRootJobs=NO ExclusiveUser=NO GraceTime=0 Hidden=NO MaxNodes=1 MaxTime=00:15:00 MinNodes=1 LLN=NO MaxCPUsPerNode=UNLIMITED MaxCPUsPerSocket=UNLIMITED NodeSets=dev Nodes=ravg1002 PriorityJobFactor=1 PriorityTier=1 RootOnly=NO ReqResv=NO OverSubscribe=NO OverTimeLimit=NONE PreemptMode=OFF State=UP TotalCPUs=144 TotalNodes=1 SelectTypeParameters=NONE JobDefaults=DefCpuPerGPU=36 DefMemPerNode=125000 MaxMemPerNode=UNLIMITED TRES=cpu=144,mem=500000M,node=1,billing=144,gres/gpu=4,gres/gpu:a100=4 Note the MaxTime=00:15:00 property.","title":"How do I find out about the properties of SLURM partitions?"},{"location":"cluster/info/#can-i-shareaccess-data-of-other-users","text":"Yes. We usually share project data in /ptmp/myuser using setfacl and getfacl . Please read here for more information .","title":"Can I share/access data of other users?"},{"location":"cluster/info/#slurm-whats-the-point-in-specifying-memory-cores-and-time-requirements","text":"Short answer: The less you specify, the quicker your job gets scheduled. On a system like the HPC, many users compete for recourses. The SLURM scheduler tries to find a spot for your job that is as close to your requirements as possible. If it can fit your job into a spot, because, e.g., another user doesn't need a whole node and your job only needs a bit of computational power, then it will do so. On the other hand, if you specify a lot of memory or a lot of cores, then the scheduler might need to put you at the back of the queue. Therefore, always specify the least you need.","title":"SLURM: What's the point in specifying memory, cores, and time requirements?"},{"location":"cluster/info/#whats-the-difference-between-raven-and-viper","text":"One answer is: Raven is older and Viper is newer. The other answer is: Raven has Intel CPUs and NVidia A100 GPUs, while Viper has AMD CPUs and GPUs. From a practical point of view, the Viper system is interesting because the AMD architecture offers shared memory between CPU and GPU cores (AMD calls these things APU), and it has 128GB per node. When you have a GPU accelerated program and you are fine with 40GB NVidia A100 cards, then use Raven. Otherwise, it is worth trying Viper.","title":"What's the difference between Raven and Viper?"},{"location":"contribute/contributing/","text":"Contributing to Brain Segmentation \u00b6 The most important thing first: Note We always document workflows, code, and tools Reproducibility is a cornerstone of scientific research, and it is essential that all workflows, tools, and processes used in this project are thoroughly documented. This ensures that other researchers can accurately reproduce experiments and validate the findings. Clear and detailed documentation enables new contributors to understand the methodologies and replicate the results without ambiguity. Whether it involves software dependencies, data preprocessing steps, or execution instructions, every aspect must be described comprehensively.","title":"Overview"},{"location":"contribute/contributing/#contributing-to-brain-segmentation","text":"The most important thing first: Note We always document workflows, code, and tools Reproducibility is a cornerstone of scientific research, and it is essential that all workflows, tools, and processes used in this project are thoroughly documented. This ensures that other researchers can accurately reproduce experiments and validate the findings. Clear and detailed documentation enables new contributors to understand the methodologies and replicate the results without ambiguity. Whether it involves software dependencies, data preprocessing steps, or execution instructions, every aspect must be described comprehensively.","title":"Contributing to Brain Segmentation"},{"location":"contribute/howto_write_code/","text":"Rules for Writing Code \u00b6 There are a few rules when you write code that need to be followed: We write unit tests wherever possible, and all tests must pass before code is merged. We develop on branches and use pull requests and code reviews. Variable names and code documentation must always be in English. We always try to use Python type hints for your functions and methods We always write docstrings for functions, classes, modules, etc. Use Google style documentation . 1. Why We Write Unit Tests Wherever Possible \u00b6 Unit tests are fundamental to maintaining code quality and preventing regression bugs. They serve multiple critical purposes: They verify that individual components work as expected They act as documentation by showing how code should be used They make it safer to refactor code by quickly catching unintended changes They help catch bugs early in the development process They reduce the cost of fixing bugs by identifying issues before they reach production 2. Why We Use Branches and Pull Requests \u00b6 Working with branches and conducting code reviews is essential for maintaining code quality and team collaboration: Branches allow developers to work on features or fixes without affecting the main codebase Pull requests provide a formal process for code integration Code reviews help catch bugs and design issues early Team members can learn from each other's code and share knowledge It maintains a clean and stable main branch It creates documentation of why changes were made through PR descriptions and review comments 3. Why We Use English for Variables and Documentation \u00b6 Using English throughout the codebase ensures: Global accessibility and understanding of the code Consistency across the entire codebase Easier collaboration with international team members Better integration with most programming languages and libraries, which use English keywords Simpler maintenance and support as English is the de facto standard in programming 4. Why We Use Type Hints \u00b6 Type hints improve code quality and development experience by: Making code more self-documenting Enabling better IDE support with accurate code completion and refactoring Catching type-related bugs before runtime Making it easier for new developers to understand function interfaces Improving maintainability by clarifying code intentions Supporting static type checking tools like mypy 5. Why We Write Docstrings \u00b6 Docstrings using Google style are crucial because they: Are automatically included in the documentation website Provide clear and standardized documentation for code components Help developers understand how to use functions, classes, and modules Enable automatic documentation generation Make code more maintainable by explaining complex logic or algorithms Support IDE features like hover documentation and quick help Create a consistent documentation style across the project Help new team members get up to speed quickly","title":"Writing Code"},{"location":"contribute/howto_write_code/#rules-for-writing-code","text":"There are a few rules when you write code that need to be followed: We write unit tests wherever possible, and all tests must pass before code is merged. We develop on branches and use pull requests and code reviews. Variable names and code documentation must always be in English. We always try to use Python type hints for your functions and methods We always write docstrings for functions, classes, modules, etc. Use Google style documentation .","title":"Rules for Writing Code"},{"location":"contribute/howto_write_code/#1-why-we-write-unit-tests-wherever-possible","text":"Unit tests are fundamental to maintaining code quality and preventing regression bugs. They serve multiple critical purposes: They verify that individual components work as expected They act as documentation by showing how code should be used They make it safer to refactor code by quickly catching unintended changes They help catch bugs early in the development process They reduce the cost of fixing bugs by identifying issues before they reach production","title":"1. Why We Write Unit Tests Wherever Possible"},{"location":"contribute/howto_write_code/#2-why-we-use-branches-and-pull-requests","text":"Working with branches and conducting code reviews is essential for maintaining code quality and team collaboration: Branches allow developers to work on features or fixes without affecting the main codebase Pull requests provide a formal process for code integration Code reviews help catch bugs and design issues early Team members can learn from each other's code and share knowledge It maintains a clean and stable main branch It creates documentation of why changes were made through PR descriptions and review comments","title":"2. Why We Use Branches and Pull Requests"},{"location":"contribute/howto_write_code/#3-why-we-use-english-for-variables-and-documentation","text":"Using English throughout the codebase ensures: Global accessibility and understanding of the code Consistency across the entire codebase Easier collaboration with international team members Better integration with most programming languages and libraries, which use English keywords Simpler maintenance and support as English is the de facto standard in programming","title":"3. Why We Use English for Variables and Documentation"},{"location":"contribute/howto_write_code/#4-why-we-use-type-hints","text":"Type hints improve code quality and development experience by: Making code more self-documenting Enabling better IDE support with accurate code completion and refactoring Catching type-related bugs before runtime Making it easier for new developers to understand function interfaces Improving maintainability by clarifying code intentions Supporting static type checking tools like mypy","title":"4. Why We Use Type Hints"},{"location":"contribute/howto_write_code/#5-why-we-write-docstrings","text":"Docstrings using Google style are crucial because they: Are automatically included in the documentation website Provide clear and standardized documentation for code components Help developers understand how to use functions, classes, and modules Enable automatic documentation generation Make code more maintainable by explaining complex logic or algorithms Support IDE features like hover documentation and quick help Create a consistent documentation style across the project Help new team members get up to speed quickly","title":"5. Why We Write Docstrings"},{"location":"contribute/howto_write_docs/","text":"How to write documentation \u00b6 This guide explains how to write and contribute to the documentation for the Brain Segmentation project. It covers the documentation structure, how the configuration works, and how the documentation is automatically built and deployed. Documentation Structure \u00b6 The documentation for this project is built using MkDocs , a fast and simple static site generator that's geared towards building project documentation. The documentation source files are written in Markdown and are stored in the docs/ directory of the repository. Understanding the MkDocs Configuration \u00b6 The mkdocs.yml file at the root of the repository is the configuration file for MkDocs. It defines: Basic information : Site name, description, and author Repository information : Repository name, URL, and edit URI Theme : The visual theme for the documentation (we use ReadTheDocs) Markdown extensions : Additional Markdown features enabled Navigation structure : The organization of pages in the documentation Plugins : Additional functionality for the documentation When you want to add a new documentation page, you need to: Create a Markdown file in the docs/ directory or a subdirectory Add the page to the nav section in mkdocs.yml if you want it to appear in the navigation menu Automatic Documentation Building with GitHub Actions \u00b6 One of the powerful features of our documentation setup is that it's automatically built and deployed whenever changes are pushed to the main branch. This is done using GitHub Actions , a continuous integration and continuous deployment (CI/CD) platform. How it Works \u00b6 Trigger : When someone pushes changes to the main branch Build Environment : GitHub Actions sets up a virtual machine with Ubuntu Setup : Python is installed, along with MkDocs Build and Deploy : MkDocs builds the documentation and deploys it to GitHub Pages The entire process is defined in the .github/workflows/mkdocs.yml file: What This Means for Contributors \u00b6 As a contributor, you don't need to worry about manually building or deploying the documentation. When your changes are merged into the main branch: GitHub Actions automatically detects the changes It builds the documentation using MkDocs It deploys the built documentation to the gh-pages branch GitHub Pages serves the documentation from the gh-pages branch This means you can focus on writing good documentation content without worrying about the technical details of deployment. Writing Documentation \u00b6 When writing documentation: Use Markdown : All documentation is written in Markdown format Follow the structure : Place your files in the appropriate directories Update navigation : Add your page to the nav section in mkdocs.yml if needed Preview locally : You can run mkdocs serve locally to preview changes Commit and push : Once you're satisfied, commit your changes and create a pull request Rules for Writing \u00b6 There is a small set of rules that are non-negotiable: Always write in English and use spell and grammar check. In PyCharm you can use the Grazie plugin for checking your English. Start a new line after the end of each sentence. This makes it much easier to see differences in pull-requests. Always remember, you are writing documentation for people who don't know how things work. Be precise and clear in your writing. Your future You will thank you. Markdown Tips \u00b6 MkDocs supports standard Markdown syntax plus some extensions: # Heading 1 ## Heading 2 *Italic text* **Bold text** - Bullet point - Another bullet point 1. Numbered item 2. Another numbered item [Link text](https://example.com) ![Image alt text](path/to/image.png) `inline code` !!! note This is an admonition box for notes Testing Your Documentation Locally \u00b6 Before submitting your changes, it's a good idea to preview them locally: Either use the conda environment from python_setup/environment.yml or install MkDocs: pip install mkdocs Navigate to the repository root Run mkdocs serve Open your browser to http://127.0.0.1:8000/ This will give you a live preview of the documentation that updates as you make changes.","title":"Writing Documentation"},{"location":"contribute/howto_write_docs/#how-to-write-documentation","text":"This guide explains how to write and contribute to the documentation for the Brain Segmentation project. It covers the documentation structure, how the configuration works, and how the documentation is automatically built and deployed.","title":"How to write documentation"},{"location":"contribute/howto_write_docs/#documentation-structure","text":"The documentation for this project is built using MkDocs , a fast and simple static site generator that's geared towards building project documentation. The documentation source files are written in Markdown and are stored in the docs/ directory of the repository.","title":"Documentation Structure"},{"location":"contribute/howto_write_docs/#understanding-the-mkdocs-configuration","text":"The mkdocs.yml file at the root of the repository is the configuration file for MkDocs. It defines: Basic information : Site name, description, and author Repository information : Repository name, URL, and edit URI Theme : The visual theme for the documentation (we use ReadTheDocs) Markdown extensions : Additional Markdown features enabled Navigation structure : The organization of pages in the documentation Plugins : Additional functionality for the documentation When you want to add a new documentation page, you need to: Create a Markdown file in the docs/ directory or a subdirectory Add the page to the nav section in mkdocs.yml if you want it to appear in the navigation menu","title":"Understanding the MkDocs Configuration"},{"location":"contribute/howto_write_docs/#automatic-documentation-building-with-github-actions","text":"One of the powerful features of our documentation setup is that it's automatically built and deployed whenever changes are pushed to the main branch. This is done using GitHub Actions , a continuous integration and continuous deployment (CI/CD) platform.","title":"Automatic Documentation Building with GitHub Actions"},{"location":"contribute/howto_write_docs/#how-it-works","text":"Trigger : When someone pushes changes to the main branch Build Environment : GitHub Actions sets up a virtual machine with Ubuntu Setup : Python is installed, along with MkDocs Build and Deploy : MkDocs builds the documentation and deploys it to GitHub Pages The entire process is defined in the .github/workflows/mkdocs.yml file:","title":"How it Works"},{"location":"contribute/howto_write_docs/#what-this-means-for-contributors","text":"As a contributor, you don't need to worry about manually building or deploying the documentation. When your changes are merged into the main branch: GitHub Actions automatically detects the changes It builds the documentation using MkDocs It deploys the built documentation to the gh-pages branch GitHub Pages serves the documentation from the gh-pages branch This means you can focus on writing good documentation content without worrying about the technical details of deployment.","title":"What This Means for Contributors"},{"location":"contribute/howto_write_docs/#writing-documentation","text":"When writing documentation: Use Markdown : All documentation is written in Markdown format Follow the structure : Place your files in the appropriate directories Update navigation : Add your page to the nav section in mkdocs.yml if needed Preview locally : You can run mkdocs serve locally to preview changes Commit and push : Once you're satisfied, commit your changes and create a pull request","title":"Writing Documentation"},{"location":"contribute/howto_write_docs/#rules-for-writing","text":"There is a small set of rules that are non-negotiable: Always write in English and use spell and grammar check. In PyCharm you can use the Grazie plugin for checking your English. Start a new line after the end of each sentence. This makes it much easier to see differences in pull-requests. Always remember, you are writing documentation for people who don't know how things work. Be precise and clear in your writing. Your future You will thank you.","title":"Rules for Writing"},{"location":"contribute/howto_write_docs/#markdown-tips","text":"MkDocs supports standard Markdown syntax plus some extensions: # Heading 1 ## Heading 2 *Italic text* **Bold text** - Bullet point - Another bullet point 1. Numbered item 2. Another numbered item [Link text](https://example.com) ![Image alt text](path/to/image.png) `inline code` !!! note This is an admonition box for notes","title":"Markdown Tips"},{"location":"contribute/howto_write_docs/#testing-your-documentation-locally","text":"Before submitting your changes, it's a good idea to preview them locally: Either use the conda environment from python_setup/environment.yml or install MkDocs: pip install mkdocs Navigate to the repository root Run mkdocs serve Open your browser to http://127.0.0.1:8000/ This will give you a live preview of the documentation that updates as you make changes.","title":"Testing Your Documentation Locally"},{"location":"training_data/FullHeadSeg/","text":"FullHead Segmentation Pipeline Documentation \u00b6 This document provides a detailed overview of the FullHead segmentation pipeline, combining SynthSeg and CHARM segmentations from brain MRI datasets to create consistent overlay segmentation maps. Overview \u00b6 The pipeline integrates two advanced segmentation methods: SynthSeg: A robust segmentation method using machine learning, particularly suited for clinical datasets. CHARM (SimNIBS): A segmentation tool primarily used for head modeling in neurostimulation applications. The combination ensures precise and consistent segmentation, suitable for generating training data or subsequent analyses. Usage \u00b6 Prerequisites \u00b6 Ensure the following software and environments are set up: Python 3.8 or newer Conda environments: synth_seg_orig_py38 simnibs_env For FullHead segmentation, ensure the availability of both SynthSeg and CHARM environments. SynthSeg script and directory paths: Path to SynthSeg_predict.py script Path to SynthSeg root directory Running the Pipeline \u00b6 Execute the pipeline via the command line: python create_fh_seg.py \\ --input_dir <input_dir> \\ --output_dir <output_dir> \\ --synthseg_script <synthseg_script> \\ --synthseg_dir <synthseg_dir> \\ --synthseg_env <synthseg_env> \\ --charm_env <charm_env> Arguments: \u00b6 --input_dir : Path to the directory containing input NIfTI files ( .nii or .nii.gz ). --output_dir : Path to the directory where the generated segmentation overlays will be saved. --synthseg_script : Absolute path to the SynthSeg_predict.py script. --synthseg_dir : Absolute path to the SynthSeg root directory. --synthseg_env : Name of the conda environment used for running SynthSeg. --charm_env : Name of the conda environment used for running CHARM (SimNIBS). Example \u00b6 python create_fh_seg.py \\ --input_dir /path/to/input_dir \\ --output_dir /path/to/output_dir \\ --synthseg_script /path/to/SynthSeg_predict.py \\ --synthseg_dir /path/to/SynthSeg \\ --synthseg_env synth_seg_orig_py38 \\ --charm_env simnibs_env Pipeline Steps \u00b6 The pipeline performs the following sequential steps: SynthSeg Segmentation: Automatically segments brain structures robustly from the input MRI. CHARM Segmentation: Performs head model segmentation, including specific anatomical labels. Label Isolation and Resampling: Specific CHARM labels (501, 502, 506, 507, 508, 509, 511, 512, 514, 515, 516, 517, 520, 530) are isolated, and SynthSeg segmentation is resampled to match the CHARM segmentation resolution. Overlay Creation: Combines both segmentations into a unified segmentation map, with SynthSeg labels taking precedence over CHARM labels where they overlap (SynthSeg labels are used wherever they exist, and CHARM labels are used only in areas where SynthSeg does not provide a label). Label Consistency Verification: Ensures all generated overlays share identical sets of segmentation labels. Output Files \u00b6 Final Output \u00b6 The main output overlays are stored with filenames structured as: <basename_of_input_file>_overlay.nii.gz Intermediate Files \u00b6 The pipeline also generates several intermediate files during processing: <basename_of_input_file>_synthseg.nii.gz : SynthSeg segmentation output <basename_of_input_file>_charm/ : Directory containing CHARM segmentation results <basename_of_input_file>_charm_filtered.nii.gz : CHARM segmentation with isolated labels <basename_of_input_file>_synthseg_resampled.nii.gz : SynthSeg segmentation resampled to match CHARM resolution Label Consistency Check \u00b6 The pipeline automatically verifies label consistency across all generated segmentation overlays. If discrepancies are found, the process halts and explicitly identifies the inconsistent files. Testing \u00b6 Unit tests are provided to ensure functionality: test_isolate_labels : Confirms the isolation of specified labels within segmentation data. test_resample_to_target : Validates correct resampling of segmentations to target resolutions. test_verify_label_consistency_across_overlays : Checks consistency of segmentation labels across multiple overlay files. Ensure tests are run in environments with appropriate SynthSeg and CHARM setups for complete pipeline validation.","title":"FullHeadSeg"},{"location":"training_data/FullHeadSeg/#fullhead-segmentation-pipeline-documentation","text":"This document provides a detailed overview of the FullHead segmentation pipeline, combining SynthSeg and CHARM segmentations from brain MRI datasets to create consistent overlay segmentation maps.","title":"FullHead Segmentation Pipeline Documentation"},{"location":"training_data/FullHeadSeg/#overview","text":"The pipeline integrates two advanced segmentation methods: SynthSeg: A robust segmentation method using machine learning, particularly suited for clinical datasets. CHARM (SimNIBS): A segmentation tool primarily used for head modeling in neurostimulation applications. The combination ensures precise and consistent segmentation, suitable for generating training data or subsequent analyses.","title":"Overview"},{"location":"training_data/FullHeadSeg/#usage","text":"","title":"Usage"},{"location":"training_data/FullHeadSeg/#prerequisites","text":"Ensure the following software and environments are set up: Python 3.8 or newer Conda environments: synth_seg_orig_py38 simnibs_env For FullHead segmentation, ensure the availability of both SynthSeg and CHARM environments. SynthSeg script and directory paths: Path to SynthSeg_predict.py script Path to SynthSeg root directory","title":"Prerequisites"},{"location":"training_data/FullHeadSeg/#running-the-pipeline","text":"Execute the pipeline via the command line: python create_fh_seg.py \\ --input_dir <input_dir> \\ --output_dir <output_dir> \\ --synthseg_script <synthseg_script> \\ --synthseg_dir <synthseg_dir> \\ --synthseg_env <synthseg_env> \\ --charm_env <charm_env>","title":"Running the Pipeline"},{"location":"training_data/FullHeadSeg/#arguments","text":"--input_dir : Path to the directory containing input NIfTI files ( .nii or .nii.gz ). --output_dir : Path to the directory where the generated segmentation overlays will be saved. --synthseg_script : Absolute path to the SynthSeg_predict.py script. --synthseg_dir : Absolute path to the SynthSeg root directory. --synthseg_env : Name of the conda environment used for running SynthSeg. --charm_env : Name of the conda environment used for running CHARM (SimNIBS).","title":"Arguments:"},{"location":"training_data/FullHeadSeg/#example","text":"python create_fh_seg.py \\ --input_dir /path/to/input_dir \\ --output_dir /path/to/output_dir \\ --synthseg_script /path/to/SynthSeg_predict.py \\ --synthseg_dir /path/to/SynthSeg \\ --synthseg_env synth_seg_orig_py38 \\ --charm_env simnibs_env","title":"Example"},{"location":"training_data/FullHeadSeg/#pipeline-steps","text":"The pipeline performs the following sequential steps: SynthSeg Segmentation: Automatically segments brain structures robustly from the input MRI. CHARM Segmentation: Performs head model segmentation, including specific anatomical labels. Label Isolation and Resampling: Specific CHARM labels (501, 502, 506, 507, 508, 509, 511, 512, 514, 515, 516, 517, 520, 530) are isolated, and SynthSeg segmentation is resampled to match the CHARM segmentation resolution. Overlay Creation: Combines both segmentations into a unified segmentation map, with SynthSeg labels taking precedence over CHARM labels where they overlap (SynthSeg labels are used wherever they exist, and CHARM labels are used only in areas where SynthSeg does not provide a label). Label Consistency Verification: Ensures all generated overlays share identical sets of segmentation labels.","title":"Pipeline Steps"},{"location":"training_data/FullHeadSeg/#output-files","text":"","title":"Output Files"},{"location":"training_data/FullHeadSeg/#final-output","text":"The main output overlays are stored with filenames structured as: <basename_of_input_file>_overlay.nii.gz","title":"Final Output"},{"location":"training_data/FullHeadSeg/#intermediate-files","text":"The pipeline also generates several intermediate files during processing: <basename_of_input_file>_synthseg.nii.gz : SynthSeg segmentation output <basename_of_input_file>_charm/ : Directory containing CHARM segmentation results <basename_of_input_file>_charm_filtered.nii.gz : CHARM segmentation with isolated labels <basename_of_input_file>_synthseg_resampled.nii.gz : SynthSeg segmentation resampled to match CHARM resolution","title":"Intermediate Files"},{"location":"training_data/FullHeadSeg/#label-consistency-check","text":"The pipeline automatically verifies label consistency across all generated segmentation overlays. If discrepancies are found, the process halts and explicitly identifies the inconsistent files.","title":"Label Consistency Check"},{"location":"training_data/FullHeadSeg/#testing","text":"Unit tests are provided to ensure functionality: test_isolate_labels : Confirms the isolation of specified labels within segmentation data. test_resample_to_target : Validates correct resampling of segmentations to target resolutions. test_verify_label_consistency_across_overlays : Checks consistency of segmentation labels across multiple overlay files. Ensure tests are run in environments with appropriate SynthSeg and CHARM setups for complete pipeline validation.","title":"Testing"},{"location":"training_data/charm/","text":"Charm Segmentation \u00b6 This document provides a detailed overview of the CHARM (SimNIBS) segmentation installation, execution, and setup. Description of the CHARM Method \u00b6 The SimNIBS package is usually used to calculate electric fields caused by Transcranial Electrical Stimulation (TES) and Transcranial Magnetic Stimulation (TMS). For this project, we will only use the full head segmentation functionality of SimNIBS to get labeled images for the regions outside the brain. Later we will merge these outer parts with the brain segmentation from a different, high-quality algorithm. Pipeline Steps \u00b6 The complete SimNIBS pipeline is divided into three parts: Automatic segmentation of MRI images and meshing to create individualized head models Calculation of electric fields through the Finite Element Method (FEM) Post-processing of results for further analysis. For this project, we will only use the first step. Environment Preparation \u00b6 Prerequisites \u00b6 Ensure that the following software is present: Python 3.8 or newer Installed miniforge Environment Installation \u00b6 The first step is to clone the latest release of the SimNIBS repository from GitHub. Warning When building an Apptainer, we recommend using the latest release of SimNIBS. We had problems with version 4.5.0., which resulted in a compilation bug that could be traced back to the wrong Boost library. git clone --depth 1 https://github.com/simnibs/simnibs.git --branch v4.5.0 To install SimNIBS into a new environment, use the following steps: cd simnibs source conda_path/etc/profile.d/conda.sh source conda_path/etc/profile.d/mamba.sh mamba env create -f environment_linux.yml # we assume a Linux environment here! conda activate simnibs_env pip install -e . python simnibs/cli/link_external_progs.py # If you need fsleyes for viewing, uncomment the next line # pip install fsleyes Running the Segmentation \u00b6 After installing SimNIBS, you should verify that it works correctly. To do this, use an example Nifti file with .nii extension and execute the following command: charm \"file_id\" image/path/example.nii where file_id is a name chosen by the user for this segmentation (it is recommended to use the same name as the MRI file that needs to be segmented). image/path/example.nii is the path to the MRI file that needs to be segmented. Inputs of Segmentation \u00b6 example.nii - a file with the MRI image that needs to be segmented. Outputs of Segmentation \u00b6 m2m_file_id - A directory containing all the information about segmentation and electrode connections. labeling.nii.gz - A specific file required for further work, located in the m2m_file_id directory at the path image/path/m2m_file_id/segmentation/labeling.nii.gz Example \u00b6 The following Python code demonstrates how to segment all MRI images in a specified directory: import os import time #Enter into the variables the path to a directory with MRI files path = \"/example/path/Try_A/Dir_Seg\" path_res = path + \"/../Dir_Seg_Res\" os.system(\"mkdir \" + path_res) for file in os.listdir(path): time.sleep(1) name = os.path.basename(file).split('.')[0] com = \"touch \" + path + \"/Process.txt; charm \" + name + \" \" + os.path.abspath(file) + \"; mv \" + path + \"/m2m_\" + name + \" \" + path_res + \"; rm -f \" + path + \"/Process.txt\" os.system(com) time.sleep(1) while os.path.exists(path + \"/Process.txt\"): time.sleep(5) To run the code: Enter the path to the directory containing the MRI files into the \"path\" variable in the code. Run the \"python\" command in the SimNIBS environment in the Terminal. Viewing Results \u00b6 After the segmentation is complete, you can view the results using the FSLEyes program. For the directory that contains the results of the MRI image segmentation, enter the following command: fsleyes #PATH/m2m_\"Name of File\"/segmentation/labeling.nii.gz This launches the FSLEyes program to view the segmentation results, where #PATH is the path to the directory with the results. Interface of FSLEyes \u00b6 In the opened window, you can see the results and adjust the settings as needed. To clearly view the organs from the CHARM segmentation, use the following settings in the top of the window: In \"Labeling\", select \"Label Image\" to see each organ with its particular label. Set \"Outline width\" to be thick enough so that the boundary lines are clearly visible without blocking any details. In \"Look-up Table\", use Random (big) or FreeSurferColorLut to see all the head segmentation with all organs clearly and brightly. The rest of the settings: \"Brightness\", \"Contrast\", \"Zoom\", \"Opacity\", can be adjusted according to your preference, but these values are recommended: Brightness: Middle Contrast: Middle Zoom: 100 Opacity: Maximum (the right end)","title":"CHARM Segmentation"},{"location":"training_data/charm/#charm-segmentation","text":"This document provides a detailed overview of the CHARM (SimNIBS) segmentation installation, execution, and setup.","title":"Charm Segmentation"},{"location":"training_data/charm/#description-of-the-charm-method","text":"The SimNIBS package is usually used to calculate electric fields caused by Transcranial Electrical Stimulation (TES) and Transcranial Magnetic Stimulation (TMS). For this project, we will only use the full head segmentation functionality of SimNIBS to get labeled images for the regions outside the brain. Later we will merge these outer parts with the brain segmentation from a different, high-quality algorithm.","title":"Description of the CHARM Method"},{"location":"training_data/charm/#pipeline-steps","text":"The complete SimNIBS pipeline is divided into three parts: Automatic segmentation of MRI images and meshing to create individualized head models Calculation of electric fields through the Finite Element Method (FEM) Post-processing of results for further analysis. For this project, we will only use the first step.","title":"Pipeline Steps"},{"location":"training_data/charm/#environment-preparation","text":"","title":"Environment Preparation"},{"location":"training_data/charm/#prerequisites","text":"Ensure that the following software is present: Python 3.8 or newer Installed miniforge","title":"Prerequisites"},{"location":"training_data/charm/#environment-installation","text":"The first step is to clone the latest release of the SimNIBS repository from GitHub. Warning When building an Apptainer, we recommend using the latest release of SimNIBS. We had problems with version 4.5.0., which resulted in a compilation bug that could be traced back to the wrong Boost library. git clone --depth 1 https://github.com/simnibs/simnibs.git --branch v4.5.0 To install SimNIBS into a new environment, use the following steps: cd simnibs source conda_path/etc/profile.d/conda.sh source conda_path/etc/profile.d/mamba.sh mamba env create -f environment_linux.yml # we assume a Linux environment here! conda activate simnibs_env pip install -e . python simnibs/cli/link_external_progs.py # If you need fsleyes for viewing, uncomment the next line # pip install fsleyes","title":"Environment Installation"},{"location":"training_data/charm/#running-the-segmentation","text":"After installing SimNIBS, you should verify that it works correctly. To do this, use an example Nifti file with .nii extension and execute the following command: charm \"file_id\" image/path/example.nii where file_id is a name chosen by the user for this segmentation (it is recommended to use the same name as the MRI file that needs to be segmented). image/path/example.nii is the path to the MRI file that needs to be segmented.","title":"Running the Segmentation"},{"location":"training_data/charm/#inputs-of-segmentation","text":"example.nii - a file with the MRI image that needs to be segmented.","title":"Inputs of Segmentation"},{"location":"training_data/charm/#outputs-of-segmentation","text":"m2m_file_id - A directory containing all the information about segmentation and electrode connections. labeling.nii.gz - A specific file required for further work, located in the m2m_file_id directory at the path image/path/m2m_file_id/segmentation/labeling.nii.gz","title":"Outputs of Segmentation"},{"location":"training_data/charm/#example","text":"The following Python code demonstrates how to segment all MRI images in a specified directory: import os import time #Enter into the variables the path to a directory with MRI files path = \"/example/path/Try_A/Dir_Seg\" path_res = path + \"/../Dir_Seg_Res\" os.system(\"mkdir \" + path_res) for file in os.listdir(path): time.sleep(1) name = os.path.basename(file).split('.')[0] com = \"touch \" + path + \"/Process.txt; charm \" + name + \" \" + os.path.abspath(file) + \"; mv \" + path + \"/m2m_\" + name + \" \" + path_res + \"; rm -f \" + path + \"/Process.txt\" os.system(com) time.sleep(1) while os.path.exists(path + \"/Process.txt\"): time.sleep(5) To run the code: Enter the path to the directory containing the MRI files into the \"path\" variable in the code. Run the \"python\" command in the SimNIBS environment in the Terminal.","title":"Example"},{"location":"training_data/charm/#viewing-results","text":"After the segmentation is complete, you can view the results using the FSLEyes program. For the directory that contains the results of the MRI image segmentation, enter the following command: fsleyes #PATH/m2m_\"Name of File\"/segmentation/labeling.nii.gz This launches the FSLEyes program to view the segmentation results, where #PATH is the path to the directory with the results.","title":"Viewing Results"},{"location":"training_data/charm/#interface-of-fsleyes","text":"In the opened window, you can see the results and adjust the settings as needed. To clearly view the organs from the CHARM segmentation, use the following settings in the top of the window: In \"Labeling\", select \"Label Image\" to see each organ with its particular label. Set \"Outline width\" to be thick enough so that the boundary lines are clearly visible without blocking any details. In \"Look-up Table\", use Random (big) or FreeSurferColorLut to see all the head segmentation with all organs clearly and brightly. The rest of the settings: \"Brightness\", \"Contrast\", \"Zoom\", \"Opacity\", can be adjusted according to your preference, but these values are recommended: Brightness: Middle Contrast: Middle Zoom: 100 Opacity: Maximum (the right end)","title":"Interface of FSLEyes"},{"location":"training_data/overview/","text":"Creating Training Data for Model Training \u00b6 What Datasets to Use \u00b6 Creating Full Head Segmentation \u00b6 Include references to Charm and SynthSeg segmentation. Label Map Upsampling \u00b6 Statistical Analysis of Scans \u00b6 Creating Synthetic Brain Generator Config \u00b6 Creating Training Data on HPC \u00b6","title":"General Approach"},{"location":"training_data/overview/#creating-training-data-for-model-training","text":"","title":"Creating Training Data for Model Training"},{"location":"training_data/overview/#what-datasets-to-use","text":"","title":"What Datasets to Use"},{"location":"training_data/overview/#creating-full-head-segmentation","text":"Include references to Charm and SynthSeg segmentation.","title":"Creating Full Head Segmentation"},{"location":"training_data/overview/#label-map-upsampling","text":"","title":"Label Map Upsampling"},{"location":"training_data/overview/#statistical-analysis-of-scans","text":"","title":"Statistical Analysis of Scans"},{"location":"training_data/overview/#creating-synthetic-brain-generator-config","text":"","title":"Creating Synthetic Brain Generator Config"},{"location":"training_data/overview/#creating-training-data-on-hpc","text":"","title":"Creating Training Data on HPC"},{"location":"training_data/synth_seg/","text":"SynthSeg (Original) Segmentation \u00b6 SynthSeg is a deep learning-based method for brain tumor segmentation. We use the original implementation from SynthSeg to create the segmentation of the brain that is subsequently combined with the outer head segmentation of CHARM to create the final full head segmentation GPU acceleration Usually, we want to run SynthSeg on a GPU for maximum speed. However, the original implementation relies on TensorFlow 2.2, which is quite old. On NVIDIA GPUs with CUDA architecture 8.0 or newer, kernels are JIT-compiled from PTX and TensorFlow can take over 30 minutes to start up. We found that running SynthSeg on multiple CPU cores instead of a single GPU is much faster and sufficient for our purposes. See SynthSeg options --cpu and --threads below for more details. Building a Python Environment for SynthSeg \u00b6 The package dependency files for SynthSeg can be found in the python_setup/SynthSeg directory. This includes both the environment.yml file for conda/mamba environment setup and the requirements.txt file for pip dependencies. First create a conda environment using miniforge and the environment.yml file: # We use mamba here, but conda is also fine cd python_setup/SynthSeg mamba env create -f environment.yml Activate the environment and use pip to install the rest of the dependencies: conda activate synth_seg_orig_py38 pip install -r requirements.txt This will set up a Python 3.8 environment with CUDA 10.1 support and all the necessary dependencies for running SynthSeg, including TensorFlow GPU 2.2.0, Keras 2.3.1, and other required packages. Running SynthSeg \u00b6 After setting up the Python environment, you can run SynthSeg using the SynthSeg_predict.py script. Below is a guide to the available options: Basic Usage \u00b6 python ./scripts/commands/SynthSeg_predict.py --i <input_image_or_folder> --o <output_folder> Command-Line Options \u00b6 Option Description --i I Input image(s) to segment. Can be a path to a single image or to a folder containing multiple images. --o O Output folder for segmentation results. Must be a folder if --i designates a folder. --parc Perform cortex parcellation in addition to basic segmentation. --robust Use robust prediction mode for higher accuracy (slower processing). --fast Bypass some postprocessing steps for faster predictions (may reduce accuracy). --ct Clip intensities to [0,80] range for CT scans. --vol VOL Path to output CSV file with volumes (mm\u00b3) for all regions and subjects. --qc QC Path to output CSV file with quality control scores for all subjects. --post POST Output folder for posterior probability maps. Must be a folder if --i designates a folder. --resample RESAMPLE Output folder for resampled images. Must be a folder if --i designates a folder. --crop CROP [CROP ...] Size of 3D patches to analyze. Default is 192. --threads THREADS Number of CPU cores to use for processing. Default is 1. --cpu Force CPU processing instead of GPU (useful when GPU memory is limited). --v1 Use SynthSeg 1.0 (updated 25/06/22). Example Commands \u00b6 Basic segmentation of a single image: python ./scripts/commands/SynthSeg_predict.py --i input_brain.nii.gz --o output_folder/ Batch processing with robust mode and volume calculation: python ./scripts/commands/SynthSeg_predict.py --i input_folder/ --o output_folder/ --robust --vol volume_stats.csv --threads 4","title":"SynthSeg Segmentation"},{"location":"training_data/synth_seg/#synthseg-original-segmentation","text":"SynthSeg is a deep learning-based method for brain tumor segmentation. We use the original implementation from SynthSeg to create the segmentation of the brain that is subsequently combined with the outer head segmentation of CHARM to create the final full head segmentation GPU acceleration Usually, we want to run SynthSeg on a GPU for maximum speed. However, the original implementation relies on TensorFlow 2.2, which is quite old. On NVIDIA GPUs with CUDA architecture 8.0 or newer, kernels are JIT-compiled from PTX and TensorFlow can take over 30 minutes to start up. We found that running SynthSeg on multiple CPU cores instead of a single GPU is much faster and sufficient for our purposes. See SynthSeg options --cpu and --threads below for more details.","title":"SynthSeg (Original) Segmentation"},{"location":"training_data/synth_seg/#building-a-python-environment-for-synthseg","text":"The package dependency files for SynthSeg can be found in the python_setup/SynthSeg directory. This includes both the environment.yml file for conda/mamba environment setup and the requirements.txt file for pip dependencies. First create a conda environment using miniforge and the environment.yml file: # We use mamba here, but conda is also fine cd python_setup/SynthSeg mamba env create -f environment.yml Activate the environment and use pip to install the rest of the dependencies: conda activate synth_seg_orig_py38 pip install -r requirements.txt This will set up a Python 3.8 environment with CUDA 10.1 support and all the necessary dependencies for running SynthSeg, including TensorFlow GPU 2.2.0, Keras 2.3.1, and other required packages.","title":"Building a Python Environment for SynthSeg"},{"location":"training_data/synth_seg/#running-synthseg","text":"After setting up the Python environment, you can run SynthSeg using the SynthSeg_predict.py script. Below is a guide to the available options:","title":"Running SynthSeg"},{"location":"training_data/synth_seg/#basic-usage","text":"python ./scripts/commands/SynthSeg_predict.py --i <input_image_or_folder> --o <output_folder>","title":"Basic Usage"},{"location":"training_data/synth_seg/#command-line-options","text":"Option Description --i I Input image(s) to segment. Can be a path to a single image or to a folder containing multiple images. --o O Output folder for segmentation results. Must be a folder if --i designates a folder. --parc Perform cortex parcellation in addition to basic segmentation. --robust Use robust prediction mode for higher accuracy (slower processing). --fast Bypass some postprocessing steps for faster predictions (may reduce accuracy). --ct Clip intensities to [0,80] range for CT scans. --vol VOL Path to output CSV file with volumes (mm\u00b3) for all regions and subjects. --qc QC Path to output CSV file with quality control scores for all subjects. --post POST Output folder for posterior probability maps. Must be a folder if --i designates a folder. --resample RESAMPLE Output folder for resampled images. Must be a folder if --i designates a folder. --crop CROP [CROP ...] Size of 3D patches to analyze. Default is 192. --threads THREADS Number of CPU cores to use for processing. Default is 1. --cpu Force CPU processing instead of GPU (useful when GPU memory is limited). --v1 Use SynthSeg 1.0 (updated 25/06/22).","title":"Command-Line Options"},{"location":"training_data/synth_seg/#example-commands","text":"Basic segmentation of a single image: python ./scripts/commands/SynthSeg_predict.py --i input_brain.nii.gz --o output_folder/ Batch processing with robust mode and volume calculation: python ./scripts/commands/SynthSeg_predict.py --i input_folder/ --o output_folder/ --robust --vol volume_stats.csv --threads 4","title":"Example Commands"},{"location":"training_data/upscaling/","text":"Upscaling Label Images \u00b6 This document provides a guide on how to use the upscaling functionality implemented in the BrainSegmentation library. Overview \u00b6 The upscaling functionality allows you to resample 3D label images (in NIfTI format) to a higher resolution. This is particularly useful when you have label images that are in a lower resolution than desired for your analysis or visualization. The upscaling process consists of two main steps: 1. Resampling : The label image is resampled to the desired resolution using nearest neighbor interpolation. 2. Optional Smoothing : If specified, a Gaussian smoothing is applied to the resampled image to reduce artifacts and improve the quality of the upscaled image. How It Works \u00b6 Resampling \u00b6 The resampling process uses nearest neighbor interpolation to resize the image to the desired resolution. This is done by: 1. Calculating the step size based on the ratio of the output resolution to the input resolution 2. Creating a grid of points in the new resolution space 3. Sampling the original image at these points using nearest neighbor interpolation Smoothing (Optional) \u00b6 If a sigma value is provided, the upscaled image undergoes Gaussian smoothing: 1. Each unique label in the image is processed separately 2. A 3D Gaussian kernel is applied in the frequency domain using Fast Fourier Transform (FFT) 3. The smoothed labels are combined based on maximum probability This smoothing helps to create more natural-looking boundaries between different labels in the upscaled image. Usage \u00b6 Command-Line Interface \u00b6 The upscaling functionality can be used directly from the command line: python -m brainseg.scale_label_image \\ --image_file /path/to/input/label_image.nii \\ --output_dir /path/to/output/directory \\ --resolution 0.5 \\ [--sigma 0.5] Parameters: - --image_file : Path to the input label image (NIfTI format) - --output_dir : Directory where the resampled image will be saved - --resolution : Desired output resolution in mm (e.g., 0.5 for 0.5mm isotropic resolution) - --sigma (optional): Standard deviation for the Gaussian kernel used for smoothing Programmatic Usage \u00b6 You can also use the upscaling functionality in your Python code: import nibabel as nib from brainseg.scale_label_image import resample_label_image # Load the input label image input_image = nib.load('/path/to/input/label_image.nii') # Resample to 0.5mm resolution resampled_image = resample_label_image( nifti=input_image, resolution_out=0.5, # Can also be a list [0.5, 0.5, 0.5] for anisotropic resolution sigma=0.5, # Optional, set to None to skip smoothing device=\"cpu\" # Use \"cuda\" for GPU acceleration if available ) # Save the resampled image nib.save(resampled_image, '/path/to/output/resampled_image.nii') Tips and Considerations \u00b6 The input image must be a 3D NIfTI image with label data Higher resolution (smaller voxel size) will result in larger output files The smoothing parameter (sigma) controls the amount of smoothing applied; larger values result in more smoothing GPU acceleration can significantly speed up the process for large images if you have a CUDA-compatible GPU","title":"Label Upscaling"},{"location":"training_data/upscaling/#upscaling-label-images","text":"This document provides a guide on how to use the upscaling functionality implemented in the BrainSegmentation library.","title":"Upscaling Label Images"},{"location":"training_data/upscaling/#overview","text":"The upscaling functionality allows you to resample 3D label images (in NIfTI format) to a higher resolution. This is particularly useful when you have label images that are in a lower resolution than desired for your analysis or visualization. The upscaling process consists of two main steps: 1. Resampling : The label image is resampled to the desired resolution using nearest neighbor interpolation. 2. Optional Smoothing : If specified, a Gaussian smoothing is applied to the resampled image to reduce artifacts and improve the quality of the upscaled image.","title":"Overview"},{"location":"training_data/upscaling/#how-it-works","text":"","title":"How It Works"},{"location":"training_data/upscaling/#resampling","text":"The resampling process uses nearest neighbor interpolation to resize the image to the desired resolution. This is done by: 1. Calculating the step size based on the ratio of the output resolution to the input resolution 2. Creating a grid of points in the new resolution space 3. Sampling the original image at these points using nearest neighbor interpolation","title":"Resampling"},{"location":"training_data/upscaling/#smoothing-optional","text":"If a sigma value is provided, the upscaled image undergoes Gaussian smoothing: 1. Each unique label in the image is processed separately 2. A 3D Gaussian kernel is applied in the frequency domain using Fast Fourier Transform (FFT) 3. The smoothed labels are combined based on maximum probability This smoothing helps to create more natural-looking boundaries between different labels in the upscaled image.","title":"Smoothing (Optional)"},{"location":"training_data/upscaling/#usage","text":"","title":"Usage"},{"location":"training_data/upscaling/#command-line-interface","text":"The upscaling functionality can be used directly from the command line: python -m brainseg.scale_label_image \\ --image_file /path/to/input/label_image.nii \\ --output_dir /path/to/output/directory \\ --resolution 0.5 \\ [--sigma 0.5] Parameters: - --image_file : Path to the input label image (NIfTI format) - --output_dir : Directory where the resampled image will be saved - --resolution : Desired output resolution in mm (e.g., 0.5 for 0.5mm isotropic resolution) - --sigma (optional): Standard deviation for the Gaussian kernel used for smoothing","title":"Command-Line Interface"},{"location":"training_data/upscaling/#programmatic-usage","text":"You can also use the upscaling functionality in your Python code: import nibabel as nib from brainseg.scale_label_image import resample_label_image # Load the input label image input_image = nib.load('/path/to/input/label_image.nii') # Resample to 0.5mm resolution resampled_image = resample_label_image( nifti=input_image, resolution_out=0.5, # Can also be a list [0.5, 0.5, 0.5] for anisotropic resolution sigma=0.5, # Optional, set to None to skip smoothing device=\"cpu\" # Use \"cuda\" for GPU acceleration if available ) # Save the resampled image nib.save(resampled_image, '/path/to/output/resampled_image.nii')","title":"Programmatic Usage"},{"location":"training_data/upscaling/#tips-and-considerations","text":"The input image must be a 3D NIfTI image with label data Higher resolution (smaller voxel size) will result in larger output files The smoothing parameter (sigma) controls the amount of smoothing applied; larger values result in more smoothing GPU acceleration can significantly speed up the process for large images if you have a CUDA-compatible GPU","title":"Tips and Considerations"}]}